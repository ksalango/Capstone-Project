{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac380faa-6705-4c9e-a5b1-12528f34a8ec",
   "metadata": {},
   "source": [
    "# Forecasting Wave Height and Wave period Ensemble Methods\n",
    "## XGBoost and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57fdbd-b532-4ac1-a3b0-d15bfd39cc43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook focuses on the independent forecasting of wave height and wave period, current features for wave height are not excluded in forecasting wave period and current values for wave period are not excluded in forecasting wave height. Results from this notebook will be compared to the results from the last notebook where wave period and wave height will be forecasted together for the same day. The differences in error will be compared. This will lead to a better understanding of the relationship between wave height and wave period in forecasting both. As seen in previous linear regression both values show importance in predicting the other. \n",
    "To forecast wave height and wave period XGBoost models as well as Random Forest models will be used. \n",
    "\n",
    "# Methodology\n",
    "\n",
    "First the daily sampled data frame for time series is imported. From there the rolling data frame is created. All directional data is converted to radians and then lags of all features are created. Features are also added. Moon phase is added and cyclicly encoded. Temporal variables are added as well such as week, season, and year, these variables are also cyclicly encoded, this may help in model interpretation of variables. Once there is a rolling data frame with added features the modelling is performed for wave height and wave period separately. Each model was optimized using gridsearch and time series split from sklearn for cross validation. Number of splits was set to n=3. The same time split for test and train was maintained throughout this process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aeec1d-96b4-4478-bf44-cdfe53fb6eb8",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "<details>\n",
    "  <summary>Data Dictionary</summary>\n",
    "  \n",
    "| Field       | Description                                             |\n",
    "|-------------|---------------------------------------------------------|\n",
    "| Index       | Date time excluding minutes (used to join df_buoy and df_hind) |\n",
    "| DEPTH       | Depth in meters                                        |\n",
    "| VWH         | Characteristic significant wave height (reported by the buoy) (m) |\n",
    "| VCMX        | Maximum zero crossing wave height (reported by the buoy) (m) |\n",
    "| VTP         | Wave spectrum peak period (reported by the buoy) (s)   |\n",
    "| WDIR        | Direction from which the wind is blowing (° True)      |\n",
    "| WDIR.1      | Estimated Direction from which the wind is blowing (° True)      |\n",
    "| WSPD        | Horizontal wind speed (m/s)                           |\n",
    "| WSPD.1      | Estimated wind speed within 10 meters. (m/s)\n",
    "| WSS         | Horizontal scalar wind speed (m/s)                     |\n",
    "| GSPD        | Gust wind speed (m/s)                                   |\n",
    "| GSPD.1      | Documentation not found            |\n",
    "| ATMS        | Atmospheric pressure at sea level (mbar)               |\n",
    "| DRYT        | Dry bulb temperature (air temperature) (°C)            |\n",
    "| SSTP        | Sea surface temperature (°C)                           |\n",
    "| WD          | Wind Direction (deg from which wind is blowing (° True)) |\n",
    "| WS          | Wind Speed (m/s)                                       |\n",
    "| ETOT        | Total Variance of Total Spectrum (m^2)                |\n",
    "| TP          | Peak Spectral Period of Total Spectrum (sec)           |\n",
    "| VMD         | Vector Mean Direction of Total Spectrum (deg to which) |\n",
    "| ETTSea      | Total Variance of Primary Partition (m^2)             |\n",
    "| TPSea       | Peak Spectral Period of Primary Partition (sec)        |\n",
    "| VMDSea      | Vector Mean Direction of Primary Partition (deg to which) |\n",
    "| ETTSw       | Total Variance of Secondary Partition (m^2)           |\n",
    "| TPSw        | Peak Spectral Period of Secondary Partition (sec)      |\n",
    "| VMDSw       | Vector Mean Direction of Secondary Partition (deg to which) |\n",
    "| MO1         | First Spectral Moment of Total Spectrum (m^2/s)       |\n",
    "| MO2         | Second Spectral Moment of Total Spectrum (m^2/s^2)    |\n",
    "| HS          | Significant Wave Height (m)                            |\n",
    "| DMDIR       | Dominant Direction (deg to which)                       |\n",
    "| ANGSPR      | Angular Spreading Function                             |\n",
    "| INLINE      | In-Line Variance Ratio                                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214a49fe-98af-4da5-8544-ce7d49f38b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import decimal\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70779be-c8d3-436c-96f5-72e056f90cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../Data/df_daily_imputed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6dc92e-5ae7-41db-a6a3-2583ba1eb1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9902 entries, 1988-11-22 to 2016-01-01\n",
      "Data columns (total 33 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   LATITUDE   9902 non-null   float64\n",
      " 1   LONGITUDE  9902 non-null   float64\n",
      " 2   DEPTH      9902 non-null   float64\n",
      " 3   VWH$       9902 non-null   float64\n",
      " 4   VCMX       9902 non-null   float64\n",
      " 5   VTP$       9902 non-null   float64\n",
      " 6   WDIR       9902 non-null   float64\n",
      " 7   WSPD       9902 non-null   float64\n",
      " 8   GSPD       9902 non-null   float64\n",
      " 9   WDIR.1     9902 non-null   float64\n",
      " 10  WSPD.1     9902 non-null   float64\n",
      " 11  GSPD.1     9902 non-null   float64\n",
      " 12  ATMS       9902 non-null   float64\n",
      " 13  DRYT       9902 non-null   float64\n",
      " 14  SSTP       9902 non-null   float64\n",
      " 15  YEAR       9902 non-null   float64\n",
      " 16  WD         9902 non-null   float64\n",
      " 17  WS         9902 non-null   float64\n",
      " 18  ETOT       9902 non-null   float64\n",
      " 19  TP         9902 non-null   float64\n",
      " 20  VMD        9902 non-null   float64\n",
      " 21  ETTSea     9902 non-null   float64\n",
      " 22  TPSea      9902 non-null   float64\n",
      " 23  VMDSea     9902 non-null   float64\n",
      " 24  ETTSw      9902 non-null   float64\n",
      " 25  TPSw       9902 non-null   float64\n",
      " 26  VMDSw      9902 non-null   float64\n",
      " 27  MO1        9902 non-null   float64\n",
      " 28  MO2        9902 non-null   float64\n",
      " 29  HS         9902 non-null   float64\n",
      " 30  DMDIR      9902 non-null   float64\n",
      " 31  ANGSPR     9902 non-null   float64\n",
      " 32  INLINE     9902 non-null   float64\n",
      "dtypes: float64(33)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc5161-a2f8-4c2a-b86f-064bba7b8903",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e1891-eccb-4c98-99e9-9d0ac1f5f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert directions(degrees North) into radians\n",
    "columns_to_convert = ['VMD', 'VMDSea', 'VMDSw', 'WD', 'WDIR', 'WDIR.1']\n",
    "\n",
    "# Convert specified columns to radians\n",
    "df[columns_to_convert] = np.radians(df[columns_to_convert])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47ec27e-851a-49e5-ac0a-5c56867b79c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               LATITUDE  LONGITUDE  DEPTH      VWH$      VCMX       VTP$  \\\n",
      "Datetime_buoy                                                              \n",
      "1989-02-20        48.83      126.0   73.0  2.000417  3.683333  12.922500   \n",
      "1989-02-21        48.83      126.0   73.0  2.281739  3.926087  13.330435   \n",
      "1989-02-22        48.83      126.0   73.0  2.645000  4.691667  10.620833   \n",
      "1989-02-23        48.83      126.0   73.0  2.488750  4.337500  10.010000   \n",
      "1989-02-24        48.83      126.0   73.0  2.564583  4.475000  12.950833   \n",
      "\n",
      "                   WDIR       WSPD       GSPD    WDIR.1  ...  \\\n",
      "Datetime_buoy                                            ...   \n",
      "1989-02-20     2.070397   9.537500  11.558333  1.914772  ...   \n",
      "1989-02-21     2.057971  10.847826  13.182609  1.891026  ...   \n",
      "1989-02-22     2.600541   8.416667  10.287500  2.431098  ...   \n",
      "1989-02-23     2.953970   5.193750   6.987500  2.781618  ...   \n",
      "1989-02-24     1.845686   5.983333   7.516667  1.693697  ...   \n",
      "\n",
      "               DMDIR_lag_1_month  DMDIR_lag_3_month  ANGSPR_lag_1_day  \\\n",
      "Datetime_buoy                                                           \n",
      "1989-02-20             94.429252          67.000000          0.797225   \n",
      "1989-02-21            101.572331          52.469565          0.832525   \n",
      "1989-02-22             96.107692          75.142857          0.773209   \n",
      "1989-02-23            163.115000          87.786957          0.637375   \n",
      "1989-02-24            154.975000          89.922727          0.684806   \n",
      "\n",
      "               ANGSPR_lag_1_week  ANGSPR_lag_1_month  ANGSPR_lag_3_month  \\\n",
      "Datetime_buoy                                                              \n",
      "1989-02-20              0.641100            0.738620            0.807500   \n",
      "1989-02-21              0.604950            0.732446            0.771422   \n",
      "1989-02-22              0.658125            0.833238            0.850457   \n",
      "1989-02-23              0.652967            0.718400            0.859587   \n",
      "1989-02-24              0.540887            0.708788            0.819845   \n",
      "\n",
      "               INLINE_lag_1_day  INLINE_lag_1_week  INLINE_lag_1_month  \\\n",
      "Datetime_buoy                                                            \n",
      "1989-02-20             0.730675           0.660987            0.727566   \n",
      "1989-02-21             0.738725           0.615088            0.726329   \n",
      "1989-02-22             0.684091           0.612550            0.756046   \n",
      "1989-02-23             0.630088           0.541333            0.702125   \n",
      "1989-02-24             0.583431           0.617443            0.668312   \n",
      "\n",
      "               INLINE_lag_3_month  \n",
      "Datetime_buoy                      \n",
      "1989-02-20               0.715300  \n",
      "1989-02-21               0.673013  \n",
      "1989-02-22               0.765200  \n",
      "1989-02-23               0.789322  \n",
      "1989-02-24               0.757150  \n",
      "\n",
      "[5 rows x 165 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/3227811285.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n"
     ]
    }
   ],
   "source": [
    "# Define lags for different time intervals\n",
    "lags = {'1_day': 1, '1_week': 7, '1_month': 30, '3_month': 90}\n",
    "\n",
    "\n",
    "# Create a new DataFrame to avoid modifying the original DataFrame in place\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# Create lags\n",
    "for column in df.select_dtypes(include='number').columns:\n",
    "    # Create lags for different time intervalsa\n",
    "    for lag_name, lag_value in lags.items():\n",
    "        new_df[f'{column}_lag_{lag_name}'] = df[column].shift(lag_value)\n",
    "\n",
    "\n",
    "# Combine the new features with the original DataFrame\n",
    "features_df = pd.concat([df, new_df], axis=1)\n",
    "\n",
    "# Drop rows with null values\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(features_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbbb199-db8f-4509-88c4-8e9857d28025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9812, 165)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c133696d-2308-461f-909d-29bcd2bc5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.index = pd.to_datetime(features_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa471d34-07f8-48af-87a0-033eb9a0d5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>VWH$</th>\n",
       "      <th>VCMX</th>\n",
       "      <th>VTP$</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GSPD</th>\n",
       "      <th>WDIR.1</th>\n",
       "      <th>...</th>\n",
       "      <th>DMDIR_lag_1_month</th>\n",
       "      <th>DMDIR_lag_3_month</th>\n",
       "      <th>ANGSPR_lag_1_day</th>\n",
       "      <th>ANGSPR_lag_1_week</th>\n",
       "      <th>ANGSPR_lag_1_month</th>\n",
       "      <th>ANGSPR_lag_3_month</th>\n",
       "      <th>INLINE_lag_1_day</th>\n",
       "      <th>INLINE_lag_1_week</th>\n",
       "      <th>INLINE_lag_1_month</th>\n",
       "      <th>INLINE_lag_3_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime_buoy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989-02-20</th>\n",
       "      <td>48.83</td>\n",
       "      <td>126.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.000417</td>\n",
       "      <td>3.683333</td>\n",
       "      <td>12.9225</td>\n",
       "      <td>2.070397</td>\n",
       "      <td>9.5375</td>\n",
       "      <td>11.558333</td>\n",
       "      <td>1.914772</td>\n",
       "      <td>...</td>\n",
       "      <td>94.429252</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.797225</td>\n",
       "      <td>0.6411</td>\n",
       "      <td>0.73862</td>\n",
       "      <td>0.8075</td>\n",
       "      <td>0.730675</td>\n",
       "      <td>0.660987</td>\n",
       "      <td>0.727566</td>\n",
       "      <td>0.7153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               LATITUDE  LONGITUDE  DEPTH      VWH$      VCMX     VTP$  \\\n",
       "Datetime_buoy                                                            \n",
       "1989-02-20        48.83      126.0   73.0  2.000417  3.683333  12.9225   \n",
       "\n",
       "                   WDIR    WSPD       GSPD    WDIR.1  ...  DMDIR_lag_1_month  \\\n",
       "Datetime_buoy                                         ...                      \n",
       "1989-02-20     2.070397  9.5375  11.558333  1.914772  ...          94.429252   \n",
       "\n",
       "               DMDIR_lag_3_month  ANGSPR_lag_1_day  ANGSPR_lag_1_week  \\\n",
       "Datetime_buoy                                                           \n",
       "1989-02-20                  67.0          0.797225             0.6411   \n",
       "\n",
       "               ANGSPR_lag_1_month  ANGSPR_lag_3_month  INLINE_lag_1_day  \\\n",
       "Datetime_buoy                                                             \n",
       "1989-02-20                0.73862              0.8075          0.730675   \n",
       "\n",
       "               INLINE_lag_1_week  INLINE_lag_1_month  INLINE_lag_3_month  \n",
       "Datetime_buoy                                                             \n",
       "1989-02-20              0.660987            0.727566              0.7153  \n",
       "\n",
       "[1 rows x 165 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8685a6f-1947-4501-bcdb-ca85ef91dae1",
   "metadata": {},
   "source": [
    "**Cyclic Encode Temporal Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea449e3-9ecb-4991-82d0-d4b5fa9ffa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['month_sin'] = np.sin(2 * np.pi * features_df.index.month / 12)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['month_cos'] = np.cos(2 * np.pi * features_df.index.month / 12)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['season_sin'] = np.sin(2 * np.pi * features_df.index.month % 12 / 4)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['season_cos'] = np.cos(2 * np.pi * features_df.index.month % 12 / 4)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['week_sin'] = np.sin(2 * np.pi * features_df.index.strftime('%U').astype(int) / 52)  # Assuming 52 weeks in a year\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/5589926.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['week_cos'] = np.cos(2 * np.pi * features_df.index.strftime('%U').astype(int) / 52)\n"
     ]
    }
   ],
   "source": [
    "#Create cyclical encoded features for month, season, and week\n",
    "features_df['month_sin'] = np.sin(2 * np.pi * features_df.index.month / 12)\n",
    "features_df['month_cos'] = np.cos(2 * np.pi * features_df.index.month / 12)\n",
    "\n",
    "#Assume seasons are defined as quarters (1-4)\n",
    "features_df['season_sin'] = np.sin(2 * np.pi * features_df.index.month % 12 / 4)\n",
    "features_df['season_cos'] = np.cos(2 * np.pi * features_df.index.month % 12 / 4)\n",
    "\n",
    "features_df['week_sin'] = np.sin(2 * np.pi * features_df.index.strftime('%U').astype(int) / 52)  # Assuming 52 weeks in a year\n",
    "features_df['week_cos'] = np.cos(2 * np.pi * features_df.index.strftime('%U').astype(int) / 52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726745b2-5e87-4c1c-aa6e-277c58da2530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9812, 171)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6349fe5d-5eb8-4763-9e4d-dd2cfc73088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add moonphase as a column (Code for moonphase taken from kaggle: https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776)\n",
    "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
    "    diff = d - datetime(2001, 1, 1)\n",
    "    days = decimal.Decimal(diff.days) + (decimal.Decimal(diff.seconds) / decimal.Decimal(86400))\n",
    "    lunations = decimal.Decimal(\"0.20439731\") + (days * decimal.Decimal(\"0.03386319269\"))\n",
    "    phase_index = math.floor((lunations % decimal.Decimal(1) * decimal.Decimal(8)) + decimal.Decimal('0.5'))\n",
    "    return int(phase_index) & 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9000239-3b92-46f4-8f20-a7a356ee2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/2573859647.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['moon_phase'] = features_df.index.map(get_moon_phase)\n"
     ]
    }
   ],
   "source": [
    "features_df['moon_phase'] = features_df.index.map(get_moon_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad14f8f-efc0-4e4d-b1db-a084627317a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/2594305688.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['moon_phase_sin'] = np.sin(2 * np.pi * features_df['moon_phase'] / 8)\n",
      "/var/folders/h5/pnqm5hvd2vj1397_ck_gmtj00000gn/T/ipykernel_36713/2594305688.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features_df['moon_phase_cos'] = np.cos(2 * np.pi * features_df['moon_phase'] / 8)\n"
     ]
    }
   ],
   "source": [
    "#cyclic encode the moonphase as it is ordinal, then drop moon phase\n",
    "features_df['moon_phase_sin'] = np.sin(2 * np.pi * features_df['moon_phase'] / 8)\n",
    "features_df['moon_phase_cos'] = np.cos(2 * np.pi * features_df['moon_phase'] / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a79bdb-67f3-46ab-aa92-e014bd9451aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df=features_df.drop('moon_phase', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9228fc54-a5b7-405d-be6e-ee3ba4ab96a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9812, 173)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56375f70-0add-46f8-8f42-672d23dee878",
   "metadata": {},
   "source": [
    "# Modelling XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c809e72-ee51-4647-a118-efa9ec1c82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9402da-07e8-4a3b-ae33-39fb3434a8d5",
   "metadata": {},
   "source": [
    "## Wave Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4562ecb-f67f-4a48-b78c-84df39d2fae9",
   "metadata": {},
   "source": [
    "**Train Test Split**\n",
    "\n",
    "First a test train split will be done, on the datetime index. Since datetime index is in order 80% of the len of the data frame will be taken in order for the train set. TimeSeries Split will be used for cross validation from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86bb496b-b0f8-49a8-b630-7725219d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split on date for train and test\n",
    "split_point = '2006-01-01'\n",
    "#filter data on split point\n",
    "train_xg= features_df.index < split_point\n",
    "test_xg = features_df.index >= split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e40608-2052-4cdc-a157-e5ad4e871475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y\n",
    "X_train = features_df[train_xg].drop(['VTP$'],axis=1)\n",
    "y_train = features_df[train_xg]['VTP$']\n",
    "\n",
    "X_test = features_df[test_xg].drop(['VTP$'], axis=1)\n",
    "y_test = features_df[test_xg]['VTP$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0312a0a6-d244-4129-9074-3888bec000da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6159, 172)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696709dc-da77-4e02-a336-cbe84bbbf579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3653, 172)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d667766-19fd-459b-84bf-e766f03b58e6",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fc1b0-d94c-42cb-b0e1-ed8bc0e63c41",
   "metadata": {},
   "source": [
    " **System Specifications and Parallelization**\n",
    "\n",
    "- **Model Name:** Mac mini\n",
    "- **Chip:** Apple M2\n",
    "- **Total Number of Cores:** 8 (4 performance and 4 efficiency)\n",
    "- **Memory:** 16 GB\n",
    "\n",
    "\n",
    "**Parallization**\n",
    "n_jobs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b160dad-95a6-4963-9eee-e93e85bacca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline object\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb_model',xgb.XGBRegressor(objective ='reg:squarederror',random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f71bd52-3d1d-4d14-a972-968e76dfa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param Grid\n",
    "param_grid_gbtree = {\n",
    "    'xgb_model__booster': ['gbtree'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb_model__max_depth': [3, 5, 7],\n",
    "    'xgb_model__n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "# Param grid for gblinear booster\n",
    "param_grid_gblinear = {\n",
    "    'xgb_model__booster': ['gblinear'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb_model__reg_alpha': [0, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid =[param_grid_gbtree, param_grid_gblinear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "188e2ef0-237a-45dc-9ff7-1e6751622b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed in 46.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "#grid search\n",
    "#cv = sklearn TimeSeries Split\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "#set timer\n",
    "start_time = time.time()\n",
    "grid_search1 = GridSearchCV(xgb_pipeline, param_grid, cv=tscv, scoring='neg_mean_squared_error',n_jobs=3) #will optimize for smallest mse\n",
    "grid_search1.fit(X_train, y_train)\n",
    "#end timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Grid search completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1319fae7-7ea8-4555-bfce-905074360afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score MSE: -3.253739990909796\n",
      "Best Hyperparameters:\n",
      "{'xgb_model__booster': 'gbtree', 'xgb_model__learning_rate': 0.2, 'xgb_model__max_depth': 3, 'xgb_model__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "#get the best params\n",
    "best_params = grid_search1.best_params_\n",
    "best_score1 = grid_search1.best_score_\n",
    "print('Best Score MSE:', best_score1)\n",
    "print('Best Hyperparameters:')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7224b9c2-184e-4e60-8c95-dc987dc2b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5e632-52e7-43b0-bb26-295d1f6efa85",
   "metadata": {},
   "source": [
    "Hyperparameters are at boundaries of ranges. Grid Search will be run again with an expanded hyperparameter space and just on gbtree as it was the best model. In future may come back to gblinear and try to optimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f3c805b-bbce-400f-b96e-fb2cd8266bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use function for grid search\n",
    "def run_grid_search(X_train, y_train, param_grid, pipeline, scoring_metric, identifier):\n",
    "    \"\"\"\n",
    "    Run a grid search with the specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - y_train: Training labels\n",
    "    - param_grid: Parameter grid for the grid search\n",
    "    - pipeline: pipeline object\n",
    "    - scoring_metric: Scikit-learn scoring metric\n",
    "    - identifier: Identifier for iteration of Gridsearch'\n",
    "\n",
    "    Returns:\n",
    "    - grid_search: Fitted GridSearchCV object \n",
    "    -resluts of cross validation in terms of best params, best score (validation set)\n",
    "    - time it took to run GridSearch\n",
    "    \"\"\"\n",
    "    #initiate timer module\n",
    "    start_time = time.time()\n",
    "    #time series cv\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    # Set up the scoring metric\n",
    "    scoring = scoring_metric\n",
    "\n",
    "    # Instantiate GridSearchCV with 5-fold cross-validation, n_jobs=3, and specified scoring metric\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring=scoring, n_jobs=3)\n",
    "\n",
    "    # Fit and run grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    #end timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "     # Store the results (hyperparameters and scores)\n",
    "    results_list.append({\n",
    "        'identifier': identifier,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'elapsed_time': elapsed_time\n",
    "    })\n",
    "    \n",
    "    # Print the best parameters with the identifier\n",
    "    print(f\"Best Parameters for {identifier}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Print the best score on the validation sets, \n",
    "    #.best_score_ is attribute of GridSearch CV that accesses best validation score(score specified in GS)\n",
    "    print(f\"Best {scoring_metric} Score for {identifier}: {grid_search.best_score_}\")\n",
    "    # Print the elapsed time\n",
    "    print(f\"Elapsed Time for {identifier}: {elapsed_time} seconds\")\n",
    "\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2d8f2-1c6d-4f4f-8fe8-fc38c8ef7b6d",
   "metadata": {},
   "source": [
    "**GridSearch run, Identifier = optimize_tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a84b7a-c326-493f-b595-42627128af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run gridsearch with expanded params: \n",
    "param_grid_gbtree = {\n",
    "    'xgb_model__booster': ['gbtree'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.1, 0.2, 0.5],  # Added 0.5\n",
    "    'xgb_model__max_depth': [2,3, 5, 7, 10],  # Added 2, 10\n",
    "    'xgb_model__n_estimators': [50, 100, 200, 300],  # Added 300\n",
    "    'xgb_model__subsample': [0.8, 0.9, 1.0],  # Add subsample\n",
    "    'xgb_model__colsample_bytree': [0.8, 0.9, 1.0],  # Add colsample_bytree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4ed36ee-f6db-4323-bb6f-ea9b33904862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamiasalango/anaconda3/envs/capstone_wavepower/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for optimize_tree: {'xgb_model__booster': 'gbtree', 'xgb_model__colsample_bytree': 1.0, 'xgb_model__learning_rate': 0.1, 'xgb_model__max_depth': 2, 'xgb_model__n_estimators': 200, 'xgb_model__subsample': 0.8}\n",
      "Best neg_mean_squared_error Score for optimize_tree: -3.1566080942885093\n",
      "Elapsed Time for optimize_tree: 2238.1694531440735 seconds\n"
     ]
    }
   ],
   "source": [
    "scoring_metric = 'neg_mean_squared_error'\n",
    "results_gridsearch = run_grid_search(X_train, y_train, param_grid_gbtree, xgb_pipeline, scoring_metric, 'optimize_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "455823c4-4c2e-4037-938c-63193fc13ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.60989806744921\n",
      "Mean Absolute Error: 1.1922717609930682\n"
     ]
    }
   ],
   "source": [
    "#get best model\n",
    "best_model_xg_wp =results_gridsearch.best_estimator_\n",
    "#predict\n",
    "y_pred_xg_wp = best_model_xg_wp.predict(X_test)\n",
    "mae_xg_wp = mean_absolute_error(y_test, y_pred_xg_wp)\n",
    "mse_xg_wp = mean_squared_error(y_test, y_pred_xg_wp)\n",
    "print(f'Mean Squared Error: {mse_xg_wp}')\n",
    "print(f'Mean Absolute Error: {mae_xg_wp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a005c59-0bb4-48dc-a65d-23797e8f66af",
   "metadata": {},
   "source": [
    "Appears that model is overfitting as mse of test set is lower than mse of train. Try predictions with first model from grid search, with lower learnig rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afeb8725-97f1-4c0e-8fee-b79fc06f14c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.7901761154688134\n",
      "Mean Absolute Error: 1.2144449633650984\n"
     ]
    }
   ],
   "source": [
    "#get best model\n",
    "best_model_xg_wp1 = grid_search1.best_estimator_\n",
    "#predict\n",
    "y_pred_xg_wp1 = best_model_xg_wp1.predict(X_test)\n",
    "mae_xg_wp1 = mean_absolute_error(y_test, y_pred_xg_wp1)\n",
    "mse_xg_wp1 = mean_squared_error(y_test, y_pred_xg_wp1)\n",
    "print(f'Mean Squared Error: {mse_xg_wp1}')\n",
    "print(f'Mean Absolute Error: {mae_xg_wp1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75376f98-1065-47ce-9a55-1f72b0670db0",
   "metadata": {},
   "source": [
    "Model is still overfitting, parameters will be adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345b880-d2e3-4d60-b7b6-f387f059863b",
   "metadata": {},
   "source": [
    "**GridSearch Run, Identifier 'adjust_for_ofit'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbc40b35-2669-45ee-a073-b243acbf3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_reduce_of = {\n",
    "    'xgb_model__booster': ['gbtree'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.05],  # lower the learning rate \n",
    "    'xgb_model__max_depth': [2],  #Limit max depth to 2\n",
    "    'xgb_model__reg_alpha': [0.3, 0.5, 1.0], #add alpha for regularization\n",
    "    'xgb_model__reg_lambda': [0.3, 0.5, 1.0], #add lambda for regularization\n",
    "    'xgb_model__n_estimators': [50, 100, 200, 300],  # Added 300\n",
    "    'xgb_model__subsample': [0.8, 0.9, 1.0],  # Add subsample\n",
    "    'xgb_model__colsample_bytree': [0.8, 0.9, 1.0],  # Add colsample_bytree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2eace75-86f5-41b6-a551-57775ce0e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for adjust_for_ofit: {'xgb_model__booster': 'gbtree', 'xgb_model__colsample_bytree': 0.9, 'xgb_model__learning_rate': 0.05, 'xgb_model__max_depth': 2, 'xgb_model__n_estimators': 300, 'xgb_model__reg_alpha': 1.0, 'xgb_model__reg_lambda': 1.0, 'xgb_model__subsample': 0.8}\n",
      "Best neg_mean_squared_error Score for adjust_for_ofit: -3.136918799854543\n",
      "Elapsed Time for adjust_for_ofit: 235.61184096336365 seconds\n"
     ]
    }
   ],
   "source": [
    "scoring_metric = 'neg_mean_squared_error'\n",
    "results_gridsearch_ofit = run_grid_search(X_train, y_train, param_grid_reduce_of, xgb_pipeline, scoring_metric, 'adjust_for_ofit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04f8e7-1d7f-41f1-abcd-d163b673f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_xg_wp_o1 =results_gridsearch_ofit.best_estimator_\n",
    "#predict\n",
    "y_pred_xg_wp_o1 = best_model_xg_wp_o1.predict(X_test)\n",
    "mae_xg_wp_o1 = mean_absolute_error(y_test, y_pred_xg_wp_o1)\n",
    "mse_xg_wp_o1 = mean_squared_error(y_test, y_pred_xg_wp_o1)\n",
    "print(f'Mean Squared Error: {mse_xg_wp_o1}')\n",
    "print(f'Mean Absolute Error: {mae_xg_wp_o1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01615c60-6569-4da2-9509-b2249a65585f",
   "metadata": {},
   "source": [
    "Model is still overfitting, adjust parameters further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ad129-a924-4a5e-bebf-121c8c260660",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_reduce_of2 = {\n",
    "    'xgb_model__booster': ['gbtree'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.03],  # lower the learning rate further\n",
    "    'xgb_model__max_depth': [2],  #Limit max depth to 2\n",
    "    'xgb_model__reg_alpha': [0.5, 1.0, 2.0], #increase range of alpha\n",
    "    'xgb_model__reg_lambda': [0.5, 1.0, 2.0], #increase range of lambda\n",
    "    'xgb_model__n_estimators': [50, 100, 200, 300],  # Added 300\n",
    "    'xgb_model__subsample': [0.6, 0.8, 0.9],  # lower subsampling rate\n",
    "    'xgb_model__colsample_bytree': [0.8, 0.9, 1.0],  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81b9ee-0a46-40a4-bb09-71812468e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metric = 'neg_mean_squared_error'\n",
    "results_gridsearch_ofit2 = run_grid_search(X_train, y_train, param_grid_reduce_of2, xgb_pipeline, scoring_metric, 'adjust_for_ofit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db36457-0cb1-4a24-b8e1-cfc81be90b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_xg_wp_o2 =results_gridsearch_ofit2.best_estimator_\n",
    "#predict\n",
    "y_pred_xg_wp_o2 = best_model_xg_wp_o2.predict(X_test)\n",
    "mae_xg_wp_o2 = mean_absolute_error(y_test, y_pred_xg_wp_o2)\n",
    "mse_xg_wp_o2 = mean_squared_error(y_test, y_pred_xg_wp_o2)\n",
    "print(f'Mean Squared Error: {mse_xg_wp_o1}')\n",
    "print(f'Mean Absolute Error: {mae_xg_wp_o1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f5e45-5589-4b01-bd28-372a50e6c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions vs actual \n",
    "X_test.index = pd.to_datetime(X_test.index)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "plot_data = pd.DataFrame({'Datetime': X_test.index, 'Actual': y_test, 'Predicted': y_pred_xg_wp_o2})\n",
    "\n",
    "# Plotly line plot\n",
    "fig = px.line(plot_data, x='Datetime', y=['Actual', 'Predicted'], title='Actual vs Predicted',\n",
    "              labels={'value': 'Wave Period', 'Datetime': 'Date'}, line_shape='linear')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10274b-2ea6-420e-a568-5b2b52482036",
   "metadata": {},
   "source": [
    "## Wave Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28046fa6-85a2-470b-9ace-b909e97d9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y\n",
    "X_train_wh = features_df[train_xg].drop(['VWH$'],axis=1)\n",
    "y_train_wh = features_df[train_xg]['VWH$']\n",
    "\n",
    "X_test_wh = features_df[test_xg].drop(['VWH$'], axis=1)\n",
    "y_test_wh = features_df[test_xg]['VWH$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015a161-b379-4262-b0ae-ee4d789b4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline object\n",
    "xgb_pipeline_wh = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb_model',xgb.XGBRegressor(objective ='reg:squarederror',random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679ce94-36c9-4da6-b9a1-33622d757755",
   "metadata": {},
   "source": [
    "**GridSearch Run, Identifier = wave_height_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cb2b8-8ae5-4074-9d75-c275629c3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param Grid\n",
    "param_grid_gbtree = {\n",
    "    'xgb_model__booster': ['gbtree'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb_model__max_depth': [3, 5, 7],\n",
    "    'xgb_model__n_estimators': [50, 100, 200],\n",
    "}\n",
    "\n",
    "# Param grid for gblinear booster\n",
    "param_grid_gblinear = {\n",
    "    'xgb_model__booster': ['gblinear'],\n",
    "    'xgb_model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb_model__reg_alpha': [0, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_wh =[param_grid_gbtree, param_grid_gblinear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a4443-ba0e-4810-9afb-cf21b6d4ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "scoring_metric = 'neg_mean_squared_error'\n",
    "results_gridsearch_wh = run_grid_search(X_train_wh, y_train_wh, param_grid_wh, xgb_pipeline_wh, scoring_metric, 'wave_height_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1bcbe-8d19-4750-a838-17d1cdb86ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_wh =results_gridsearch_wh.best_estimator_\n",
    "#predict\n",
    "y_pred_wh = best_model_wh.predict(X_test_wh)\n",
    "mae_wh = mean_absolute_error(y_test_wh, y_pred_wh)\n",
    "mse_wh = mean_squared_error(y_test_wh, y_pred_wh)\n",
    "print(f'Mean Squared Error: {mse_wh}')\n",
    "print(f'Mean Absolute Error: {mae_wh}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e84b53-896e-4119-9dd6-0cd6093d4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions vs actual \n",
    "X_test_wh.index = pd.to_datetime(X_test_wh.index)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "plot_data = pd.DataFrame({'Datetime': X_test_wh.index, 'Actual': y_test_wh, 'Predicted': y_pred_wh})\n",
    "\n",
    "# Plotly line plot\n",
    "fig = px.line(plot_data, x='Datetime', y=['Actual', 'Predicted'], title='Actual vs Predicted',\n",
    "              labels={'value': 'Wave Height', 'Datetime': 'Date'}, line_shape='linear')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e032eb-df88-477f-ada9-910be048b7b5",
   "metadata": {},
   "source": [
    "# Modelling Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b0a07-db20-4a5f-b64c-2b4bc7017695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb8450-35a7-4b5e-b3e8-b98afbeb08ac",
   "metadata": {},
   "source": [
    "## Wave Period\n",
    "Same train test split for wave period will be used from above: \n",
    "- X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afaa7e-20a6-4a53-b621-678cd166a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('rf_model', RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e13bea-4976-4f9d-9cb4-f34698cecb39",
   "metadata": {},
   "source": [
    "**GridSearch Run, Identifier = optimize_rf_wp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46118d-60ff-4c88-9d50-bac7ebbe087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grid\n",
    "param_grid_rf = {\n",
    "    'rf_model__n_estimators': [50, 100, 200],\n",
    "    'rf_model__max_depth': [None, 5, 10],\n",
    "    'rf_model__min_samples_split': [2, 5, 10],\n",
    "    'rf_model__min_samples_leaf': [1, 2, 4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cbf5a-28db-4e30-bbce-a9d0e4d59197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search \n",
    "scoring_metric = 'neg_mean_squared_error'\n",
    "grid_search_rf_wp = run_grid_search(X_train, y_train, param_grid_rf, rf_pipeline, scoring_metric, 'optimize_rf_wp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b74c8-73e4-48fc-b6a2-47dd053d52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_wp =grid_search_rf_wp.best_estimator_\n",
    "#predict\n",
    "y_pred_wp = best_model_wp.predict(X_test)\n",
    "mae_wp = mean_absolute_error(y_test, y_pred_wp)\n",
    "mse_wp = mean_squared_error(y_test, y_pred_wp)\n",
    "print(f'Mean Squared Error wp rf: {mse_wp}')\n",
    "print(f'Mean Absolute Error wp rf: {mae_wp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5959ea-dc3f-4bba-8021-09e163f7c940",
   "metadata": {},
   "source": [
    "The model is overfitting, parameters are adjusted as to prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954afb71-625f-411c-836c-d5d6f324123e",
   "metadata": {},
   "source": [
    "**GridSearch Run, Identifier = optimze_rf_wp1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf24ff3-e29a-418d-bba4-720d23d1c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#param grid \n",
    "param_grid_rf1 = {\n",
    "    'rf_model__n_estimators': [25, 50, 100], #decrease range of estimators\n",
    "    'rf_model__max_depth': [2, 5, 10,], #limit depth\n",
    "    'rf_model__min_samples_split': [5, 10, 20], #increase range of min sample split\n",
    "    'rf_model__min_samples_leaf': [3, 6, 12], #increase min samples per leaf , prevent leaves with few samples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805e68f-5738-47cd-bccf-dc922fec5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search \n",
    "scoring_metric = 'neg_mean_squared_error'\n",
    "grid_search_rf_wp1 = run_grid_search(X_train, y_train, param_grid_rf1, rf_pipeline, scoring_metric, 'optimize_rf_wp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9176eb2-9070-4cef-9c32-555855fef50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_wp1 =grid_search_rf_wp1.best_estimator_\n",
    "#predict\n",
    "y_pred_wp1 = best_model_wp1.predict(X_test)\n",
    "mae_wp1 = mean_absolute_error(y_test, y_pred_wp1)\n",
    "mse_wp1 = mean_squared_error(y_test, y_pred_wp1)\n",
    "print(f'Mean Squared Error wp1 rf: {mse_wp1}')\n",
    "print(f'Mean Absolute Error wp1 rf: {mae_wp1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03581d7-bfc9-42a8-ace9-08030344f8a6",
   "metadata": {},
   "source": [
    "## Wave Height\n",
    "Same train test split for wave height will be used as well as same pipeline. \n",
    "- X_train_wh, X_test_wh, y_train_wh, y_test_wh\n",
    "- rf_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66d7e2-3827-4aef-8ffd-4bf630570a24",
   "metadata": {},
   "source": [
    "**GridSearch Run, Identifier = optimize_rf_wh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36e714-57bb-4b2f-accc-c6b2dba6b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param Grid\n",
    "param_grid_rf = {\n",
    "    'rf_model__n_estimators': [50, 100, 200],\n",
    "    'rf_model__max_depth': [None, 5, 10],\n",
    "    'rf_model__min_samples_split': [2, 5, 10],\n",
    "    'rf_model__min_samples_leaf': [1, 2, 4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ade033-cd32-4223-ab93-57ac9745f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "scoring_metric = 'neg_mean_squared_error'\n",
    "grid_search_rf_wh = run_grid_search(X_train_wh, y_train_wh, param_grid_rf, rf_pipeline, scoring_metric, 'optimize_rf_wh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f922df8-8bf1-4319-9f28-5f1d39067637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model\n",
    "best_model_rf_wh =grid_search_rf_wh.best_estimator_\n",
    "#predict\n",
    "y_pred_wh = best_model_rf_wh.predict(X_test_wh)\n",
    "mae_rf_wh = mean_absolute_error(y_test_wh, y_pred_wh)\n",
    "mse_rf_wh = mean_squared_error(y_test_wh, y_pred_wh)\n",
    "print(f'Mean Squared Error wp1 rf: {mse_rf_wh}')\n",
    "print(f'Mean Absolute Error wp1 rf: {mae_rf_wh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700948ae-3a80-47fb-a902-2d623ccb719e",
   "metadata": {},
   "source": [
    "# Summary \n",
    "### Summary of Models: MSE MAE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffcfccc-e650-4444-a41c-dcf3afb12d76",
   "metadata": {},
   "source": [
    "| Model              | Wave Period (MSE) | Wave Period (MAE - seconds) | Wave Height (MSE) | Wave Height (MAE - meters) |\n",
    "| ------------------ | ------------------ | ---------------------------- | ----------------- | --------------------------- |\n",
    "| Random Forest      | 2.90              | 1.25                         | 0.015             | 0.067                       |\n",
    "| XGBoost            | 2.80              | 1.22                         | 0.017             | 0.071                       |\n",
    "| Linear Regression  | 2.83              | 1.28                         | 0.19              | 0.31                        |\n",
    "| ARIMA              | 6.94              | 2.06                         | 1.47              | 0.91                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fcf525-8368-4fc7-8c9c-544bc3bf1902",
   "metadata": {},
   "source": [
    "In Summary the best models, by lowering prediction errors were XGBoost for wave period and Random Forest for wave height. It is important to note however that with optimization the test mse was slightly higher than the train mse for all models. Further research and analysis as well as model tuning could be done to find out why. However for the purpose of this analysis the next phase of modelling will be done. Wave period and wave height will be forecasted for the same time window. Further feature refinement will be done. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28f47-3e5a-477e-b69c-9b26c7648288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_wavepower",
   "language": "python",
   "name": "capstone_wavepower"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
